{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import  layers, models,optimizers\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return ((X - np.mean(X) ) / np.std(X) )\n",
    "\n",
    "def folder_to_df(letter):\n",
    "    full_path =\"datasets/\"+ letter + \"/*.*\"  #datasets of A,B,C,D,E waas stored as folders in \"datasets\" folder\n",
    "    files = glob.glob(full_path)  #files=files=glob.glob(full_path)\n",
    "    small_df = []\n",
    "    for file in files:\n",
    "        small_df.append(pd.read_csv(file,header=None))\n",
    "    big_df = pd.concat(small_df, axis= 1)\n",
    "    return big_df.T\n",
    "\n",
    "# def folder_to_df(letter): #import the .txt files\n",
    "#     full_path =\"data/bonn_uni_datasets/\"+ letter + \"/*.*\"\n",
    "#     files = files = glob.glob(full_path)\n",
    "#     df_list = []\n",
    "#     for file in files:\n",
    "#         df_list.append(pd.read_csv(file, header = None))\n",
    "#     big_df = pd.concat(df_list, ignore_index=True, axis= 1)\n",
    "#     return big_df.T\n",
    "\n",
    "def load_as_df():\n",
    "    A = folder_to_df('A')\n",
    "    B = folder_to_df('B')\n",
    "    C = folder_to_df('C')\n",
    "    D = folder_to_df('D')\n",
    "    E = folder_to_df('E')\n",
    "    \n",
    "    normal = A.append(B).reset_index(drop = True)\n",
    "    interictal = C.append(D).reset_index(drop = True)\n",
    "    ictal = E\n",
    "\n",
    "    return normal, interictal, ictal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASET DESCRIPTION:\n",
    "This database comprises of 100 single channels EEG of 23.6 seconds with sampling rate of 173.61 Hz. Its spectral bandwidth range is between 0.5 Hz and 85 Hz. It was taken from a 128 channel acquisition system. Five patients EEG sets were cut out from a multi-channel EEG recording and named A, B, C, D and E. Set A and B are the surface EEG recorded during eyes closed and open situation of healthy patients respectively. Set C and D are the intracranial EEG recorded during a seizure free from within seizure generating area and from outside seizure generating area of epileptic patients respectively. Set E is the intracranial EEG of an epileptic patient during epileptic seizures. Each set contains 100 text files wherein each text file has 4097 samples of 1 EEG time series in ASCII code. A band pass filter with cut off frequency as 0.53 Hz and 40 Hz has been applied on the data. It is an artifact free data and hence no prior pre-processing is required for the classification of healthy (non-epileptic) and un-healthy (epileptic) signals. The strong eye movementâ€™s artefacts were omitted. It was made available in 2001. The extended version of this data is now a part of EPILEPSIA project.\n",
    "Palak Handa,Monika Mathur,Nidhi Goel \"Open and free EEG datasets for epilepsy diagnosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glob.glob(\"datasets/A/*.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal, interictal, ictal = load_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.shape,interictal.shape,ictal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1=normal.iloc[0,:]\n",
    "plt.figure(figsize=(16,3))\n",
    "plt.plot(range(len(plot1)),plot1)\n",
    "plt.title(\"Normal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2=ictal.iloc[0,:]\n",
    "plt.figure(figsize=(16,3))\n",
    "plt.plot(range(len(plot2)),plot2)\n",
    "plt.title(\"Ictal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot3=interictal.iloc[0,:]\n",
    "plt.figure(figsize=(16,3))\n",
    "plt.plot(range(len(plot3)),plot3)\n",
    "plt.title(\"Interictal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal=normalize(normal)\n",
    "interictal=normalize(interictal)\n",
    "ictal=normalize(ictal)\n",
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.loc[:,'status']=0\n",
    "ictal.loc[:,'status']=1\n",
    "interictal.loc[:,'status']=2\n",
    "all_df=pd.concat([normal,ictal,interictal],ignore_index=True)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=all_df.drop('status',axis=1)\n",
    "Y=all_df.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "# X_train=X_train/128\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=np.reshape(X_train.values,(X_train.shape[0],256,1))\n",
    "# X_test=np.reshape(X_test.values,(X_test.shape[0],128,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=models.Sequential()\n",
    "\n",
    "cnn.add(layers.Input(shape=(4097,1)))\n",
    "\n",
    "cnn.add(layers.Conv1D(filters=50,kernel_size=5,strides=3,activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "\n",
    "cnn.add(layers.Conv1D(filters=20,kernel_size=3,strides=2,activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "\n",
    "cnn.add(layers.Conv1D(filters=10,kernel_size=3,strides=2,activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "\n",
    "# cnn.add(layers.Conv1D(filters=4,kernel_size=3,strides=2,activation='relu'))\n",
    "# cnn.add(layers.BatchNormalization())\n",
    "# cnn.add(layers.MaxPooling1D(pool_size=2,strides=1, padding='valid'))\n",
    "\n",
    "\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(10, activation='relu'))\n",
    "cnn.add(layers.Dropout(0.5))\n",
    "cnn.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(X_train,Y_train,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted=cnn.predict(X_test)\n",
    "Y_predicted_arr=[]\n",
    "for i in range(100):\n",
    "    Y_predicted_arr=np.append(Y_predicted_arr,np.argmax(Y_predicted[i]))\n",
    "Y_predicted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_col=Y_test.iloc[:]\n",
    "np.array(Y_test_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix=metrics.confusion_matrix(Y_predicted_arr,Y_test)\n",
    "f,ax = plt.subplots(figsize=(6, 4))\n",
    "import seaborn as sns\n",
    "sns.heatmap(matrix, annot=True, linewidths=0.01,cmap=\"Blues\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Test Values\")\n",
    "plt.title(\"Confusion Matrix for CNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = models.Sequential()\n",
    "##without regularizer\n",
    "rnn.add(layers.LSTM(400, input_shape=(4097, 1), return_sequences=True))\n",
    "##with regularizer\n",
    "#model.add(LSTM(100, input_shape=(self.timesteps, self.data_dim), return_sequences=True, recurrent_regularizer=regularizers.l2(0.1)))\n",
    "#  model.add(Dropout(0.1))\n",
    "rnn.add(layers.TimeDistributed(layers.Dense(50)))\n",
    "rnn.add(layers.GlobalAveragePooling1D())\n",
    "#  model.add(LSTM(50, return_sequences=True, recurrent_regularizer=regularizers.l2(0.1)))\n",
    "# model.add(Flatten())\n",
    "rnn.add(layers.Dense(5, activation='softmax'))\n",
    "rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(X_train,Y_train,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
