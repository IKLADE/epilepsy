{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, BatchNormalization\n",
    "from keras import utils\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return ((X - np.mean(X) ) / np.std(X) )\n",
    "\n",
    "def folder_to_df(letter):\n",
    "    full_path =\"datasets/\"+ letter + \"/*.*\"  #datasets of A,B,C,D,E waas stored as folders in \"datasets\" folder\n",
    "    files = glob.glob(full_path)  #files=files=glob.glob(full_path)\n",
    "    small_df = []\n",
    "    for file in files:\n",
    "        small_df.append(pd.read_csv(file,header=None))\n",
    "    big_df = pd.concat(small_df, axis= 1)\n",
    "    return big_df.T\n",
    "\n",
    "# def folder_to_df(letter): #import the .txt files\n",
    "#     full_path =\"data/bonn_uni_datasets/\"+ letter + \"/*.*\"\n",
    "#     files = files = glob.glob(full_path)\n",
    "#     df_list = []\n",
    "#     for file in files:\n",
    "#         df_list.append(pd.read_csv(file, header = None))\n",
    "#     big_df = pd.concat(df_list, ignore_index=True, axis= 1)\n",
    "#     return big_df.T\n",
    "\n",
    "def load_as_df():\n",
    "    A = folder_to_df('A')\n",
    "    B = folder_to_df('B')\n",
    "    C = folder_to_df('C')\n",
    "    D = folder_to_df('D')\n",
    "    E = folder_to_df('E')\n",
    "    \n",
    "    normal = A.append(B).reset_index(drop = True)\n",
    "    interictal = C.append(D).reset_index(drop = True)\n",
    "    ictal = E\n",
    "\n",
    "    return normal, interictal, ictal\n",
    "\n",
    "def window(a, w = 512, o = 64, copy = False): #window sliding function\n",
    "    #default for training, for testing data we will split each signal in four of 1024 and apply\n",
    "    #a window size of 512 with a stride (o) of 256\n",
    "    sh = (a.size - w + 1, w)\n",
    "    st = a.strides * 2\n",
    "    view = np.lib.stride_tricks.as_strided(a, strides = st, shape = sh)[0::o]\n",
    "    if copy:\n",
    "        return view.copy()\n",
    "    else:\n",
    "        return view\n",
    "\n",
    "def enrich_train(df): #enrich data by splicing the 4097-long signals \n",
    "    #into 512 long ones with a stride of 64\n",
    "    labels = df.iloc[:,-1]\n",
    "    data = df.iloc[:, :-1]\n",
    "    res = list()\n",
    "    for i in range(len(data)):\n",
    "        res += [window(data.iloc[i].values)]\n",
    "    return res\n",
    "\n",
    "def reshape_x(arr): #shape the input data into the correct form (x1,x2,1)\n",
    "    nrows = arr.shape[0]\n",
    "    ncols = arr.shape[1]\n",
    "    return arr.reshape(nrows, ncols, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASET DESCRIPTION:\n",
    "This database comprises of 100 single channels EEG of 23.6 seconds with sampling rate of 173.61 Hz. Its spectral bandwidth range is between 0.5 Hz and 85 Hz. It was taken from a 128 channel acquisition system. Five patients EEG sets were cut out from a multi-channel EEG recording and named A, B, C, D and E. Set A and B are the surface EEG recorded during eyes closed and open situation of healthy patients respectively. Set C and D are the intracranial EEG recorded during a seizure free from within seizure generating area and from outside seizure generating area of epileptic patients respectively. Set E is the intracranial EEG of an epileptic patient during epileptic seizures. Each set contains 100 text files wherein each text file has 4097 samples of 1 EEG time series in ASCII code. A band pass filter with cut off frequency as 0.53 Hz and 40 Hz has been applied on the data. It is an artifact free data and hence no prior pre-processing is required for the classification of healthy (non-epileptic) and un-healthy (epileptic) signals. The strong eye movementâ€™s artefacts were omitted. It was made available in 2001. The extended version of this data is now a part of EPILEPSIA project.\n",
    "Palak Handa,Monika Mathur,Nidhi Goel \"Open and free EEG datasets for epilepsy diagnosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/A\\\\Z001.txt',\n",
       " 'datasets/A\\\\Z002.txt',\n",
       " 'datasets/A\\\\Z003.txt',\n",
       " 'datasets/A\\\\Z004.txt',\n",
       " 'datasets/A\\\\Z005.txt',\n",
       " 'datasets/A\\\\Z006.txt',\n",
       " 'datasets/A\\\\Z007.txt',\n",
       " 'datasets/A\\\\Z008.txt',\n",
       " 'datasets/A\\\\Z009.txt',\n",
       " 'datasets/A\\\\Z010.txt',\n",
       " 'datasets/A\\\\Z011.txt',\n",
       " 'datasets/A\\\\Z012.txt',\n",
       " 'datasets/A\\\\Z013.txt',\n",
       " 'datasets/A\\\\Z014.txt',\n",
       " 'datasets/A\\\\Z015.txt',\n",
       " 'datasets/A\\\\Z016.txt',\n",
       " 'datasets/A\\\\Z017.txt',\n",
       " 'datasets/A\\\\Z018.txt',\n",
       " 'datasets/A\\\\Z019.txt',\n",
       " 'datasets/A\\\\Z020.txt',\n",
       " 'datasets/A\\\\Z021.txt',\n",
       " 'datasets/A\\\\Z022.txt',\n",
       " 'datasets/A\\\\Z023.txt',\n",
       " 'datasets/A\\\\Z024.txt',\n",
       " 'datasets/A\\\\Z025.txt',\n",
       " 'datasets/A\\\\Z026.txt',\n",
       " 'datasets/A\\\\Z027.txt',\n",
       " 'datasets/A\\\\Z028.txt',\n",
       " 'datasets/A\\\\Z029.txt',\n",
       " 'datasets/A\\\\Z030.txt',\n",
       " 'datasets/A\\\\Z031.txt',\n",
       " 'datasets/A\\\\Z032.txt',\n",
       " 'datasets/A\\\\Z033.txt',\n",
       " 'datasets/A\\\\Z034.txt',\n",
       " 'datasets/A\\\\Z035.txt',\n",
       " 'datasets/A\\\\Z036.txt',\n",
       " 'datasets/A\\\\Z037.txt',\n",
       " 'datasets/A\\\\Z038.txt',\n",
       " 'datasets/A\\\\Z039.txt',\n",
       " 'datasets/A\\\\Z040.txt',\n",
       " 'datasets/A\\\\Z041.txt',\n",
       " 'datasets/A\\\\Z042.txt',\n",
       " 'datasets/A\\\\Z043.txt',\n",
       " 'datasets/A\\\\Z044.txt',\n",
       " 'datasets/A\\\\Z045.txt',\n",
       " 'datasets/A\\\\Z046.txt',\n",
       " 'datasets/A\\\\Z047.txt',\n",
       " 'datasets/A\\\\Z048.txt',\n",
       " 'datasets/A\\\\Z049.txt',\n",
       " 'datasets/A\\\\Z050.txt',\n",
       " 'datasets/A\\\\Z051.txt',\n",
       " 'datasets/A\\\\Z052.txt',\n",
       " 'datasets/A\\\\Z053.txt',\n",
       " 'datasets/A\\\\Z054.txt',\n",
       " 'datasets/A\\\\Z055.txt',\n",
       " 'datasets/A\\\\Z056.txt',\n",
       " 'datasets/A\\\\Z057.txt',\n",
       " 'datasets/A\\\\Z058.txt',\n",
       " 'datasets/A\\\\Z059.txt',\n",
       " 'datasets/A\\\\Z060.txt',\n",
       " 'datasets/A\\\\Z061.txt',\n",
       " 'datasets/A\\\\Z062.txt',\n",
       " 'datasets/A\\\\Z063.txt',\n",
       " 'datasets/A\\\\Z064.txt',\n",
       " 'datasets/A\\\\Z065.txt',\n",
       " 'datasets/A\\\\Z066.txt',\n",
       " 'datasets/A\\\\Z067.txt',\n",
       " 'datasets/A\\\\Z068.txt',\n",
       " 'datasets/A\\\\Z069.txt',\n",
       " 'datasets/A\\\\Z070.txt',\n",
       " 'datasets/A\\\\Z071.txt',\n",
       " 'datasets/A\\\\Z072.txt',\n",
       " 'datasets/A\\\\Z073.txt',\n",
       " 'datasets/A\\\\Z074.txt',\n",
       " 'datasets/A\\\\Z075.txt',\n",
       " 'datasets/A\\\\Z076.txt',\n",
       " 'datasets/A\\\\Z077.txt',\n",
       " 'datasets/A\\\\Z078.txt',\n",
       " 'datasets/A\\\\Z079.txt',\n",
       " 'datasets/A\\\\Z080.txt',\n",
       " 'datasets/A\\\\Z081.txt',\n",
       " 'datasets/A\\\\Z082.txt',\n",
       " 'datasets/A\\\\Z083.txt',\n",
       " 'datasets/A\\\\Z084.txt',\n",
       " 'datasets/A\\\\Z085.txt',\n",
       " 'datasets/A\\\\Z086.txt',\n",
       " 'datasets/A\\\\Z087.txt',\n",
       " 'datasets/A\\\\Z088.txt',\n",
       " 'datasets/A\\\\Z089.txt',\n",
       " 'datasets/A\\\\Z090.txt',\n",
       " 'datasets/A\\\\Z091.txt',\n",
       " 'datasets/A\\\\Z092.txt',\n",
       " 'datasets/A\\\\Z093.txt',\n",
       " 'datasets/A\\\\Z094.txt',\n",
       " 'datasets/A\\\\Z095.txt',\n",
       " 'datasets/A\\\\Z096.txt',\n",
       " 'datasets/A\\\\Z097.txt',\n",
       " 'datasets/A\\\\Z098.txt',\n",
       " 'datasets/A\\\\Z099.txt',\n",
       " 'datasets/A\\\\Z100.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"datasets/A/*.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_enrich_train(normal, interictal, ictal):\n",
    "    \n",
    "    #enrich data and reshape it to have a two dimensional array instead of three\n",
    "    normal_train_enr = np.asarray(enrich_train(normal)).reshape(-1, np.asarray(enrich_train(normal)).shape[-1])\n",
    "    interictal_train_enr = np.asarray(enrich_train(interictal)).reshape(-1, np.asarray(enrich_train(interictal)).shape[-1])\n",
    "    ictal_train_enr = np.asarray(enrich_train(ictal)).reshape(-1, np.asarray(enrich_train(ictal)).shape[-1])\n",
    "\n",
    "    #change into a dataframe to add labels easily\n",
    "    normal_train_enr_df = pd.DataFrame(normal_train_enr)\n",
    "    interictal_train_enr_df = pd.DataFrame(interictal_train_enr)\n",
    "    ictal_train_enr_df = pd.DataFrame(ictal_train_enr)\n",
    "    \n",
    "    normal_train_enr_df['labels'] = 0 # normal\n",
    "    interictal_train_enr_df['labels'] = 1 #interictal\n",
    "    ictal_train_enr_df['labels'] = 2 #ictal\n",
    "\n",
    "    #concat all\n",
    "    data_labels = pd.concat([normal_train_enr_df, interictal_train_enr_df, ictal_train_enr_df], ignore_index = True)\n",
    "    \n",
    "\n",
    "    #separates data and labels into numpy arrays for keras\n",
    "    data = data_labels.drop('labels', axis = 1).values\n",
    "    labels = data_labels.labels.values\n",
    "    \n",
    "    #labels = np.expand_dims(labels, axis=1)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal, interictal, ictal = load_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = format_enrich_train(normal, interictal, ictal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e189c16a3916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\IK\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "#Conv - 1\n",
    "model.add(Conv1D(24, 5,strides =  3, input_shape=(512,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Conv - 2\n",
    "model.add(Conv1D(16, 3,strides =  2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Conv - 3\n",
    "model.add(Conv1D(8, 3,strides =  2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#FC -1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "#Dropout\n",
    "model.add(Dropout(0.5))\n",
    "#FC -2\n",
    "model.add(Dense(3,activation = 'softmax'))\n",
    "#softmax\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "adam = tensorflow.keras.optimizers.Adam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=0.00000001, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,y,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, xtrain, ytrain, xval, yval, fold):\n",
    "    model_name = 'P-1D-CNN'\n",
    "    checkpointer = ModelCheckpoint(filepath='checkpoints/'+'fold'+ str(fold)+'.'+model_name + '.{epoch:03d}-{acc:.3f}.h5',verbose=0,monitor ='acc', save_best_only=True)\n",
    "    history = model.fit(xtrain, ytrain, batch_size=32, callbacks = [checkpointer],epochs=200, verbose = 1)\n",
    "    print(history)\n",
    "    score = model.evaluate(xval, yval, batch_size=32)\n",
    "    print('\\n')\n",
    "    print(score)\n",
    "    return score, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "X, y = format_enrich_train(normal, interictal, ictal)\n",
    "#initialize 10 fold validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "\n",
    "#10 fold cross validation loop\n",
    "# for i, (train, test) in enumerate(skf.split(X,y)):\n",
    "#     print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "start_time = time.time()\n",
    "X = reshape_x(X)\n",
    "xtrain, xval = X[train], X[test]\n",
    "ytrain, yval = y[train], y[test]\n",
    "ytrain = tensorflow.keras.utils.to_categorical(ytrain, num_classes=3, dtype='float32')\n",
    "yval = tensorflow.keras.utils.to_categorical(yval, num_classes=3, dtype='float32')\n",
    "\n",
    "\n",
    "model = None # Clearing the NN.\n",
    "model = create_model()\n",
    "# score, history = train_evaluate_model(model, xtrain, ytrain, xval, yval, i+1)\n",
    "# print(\"Ran \", i+1, \"/\", n_folds, \"Fold in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at best_model.0.966.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8c8f5d5e057a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.0.966.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at best_model.0.966.h5"
     ]
    }
   ],
   "source": [
    "best_model = load_model('best_model.0.966.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vote(df):\n",
    "    res = list()\n",
    "    for i in range(len(df)):\n",
    "        res += [window(df.iloc[i].values,w= 512, o = 256)]\n",
    "    return np.asarray(res)\n",
    "\n",
    "def count_votes(my_list): \n",
    "    freq = {} \n",
    "    for i in my_list: \n",
    "        if (i in freq): \n",
    "            freq[i] += 1\n",
    "        else: \n",
    "            freq[i] = 1\n",
    "    return freq\n",
    "\n",
    "def reshape_signal(signal):\n",
    "    signal = np.expand_dims(signal, axis=1)\n",
    "    signal = np.expand_dims(signal, axis=0)\n",
    "    return np.asarray(signal)\n",
    "\n",
    "def evaluate_subsignals(subsignals,model):\n",
    "    vote_list = np.array([])\n",
    "    for i in range(len(subsignals)):\n",
    "        mini_signal = reshape_signal(subsignals[i])\n",
    "        ynew = model.predict_classes(mini_signal)\n",
    "        vote_list = np.append(vote_list, ynew)\n",
    "    decision = count_votes(vote_list)\n",
    "    return decision_to_str(decision), vote_list\n",
    "\n",
    "def decision_to_str(dec):\n",
    "    res = list()\n",
    "    for key,val in dec.items():\n",
    "        if key == 0:\n",
    "            res += ['normal: ' + str(val) + ' votes' + '\\n']\n",
    "        if key == 1:\n",
    "            res += ['ictal: ' + str(val) + ' votes' + '\\n']\n",
    "        if key == 2:\n",
    "            res += ['interictal: ' + str(val) + ' votes' + '\\n']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_signal = split_vote(ictal_vote)\n",
    "subsignals = big_signal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision, vote_list = evaluate_subsignals(subsignals,best_model)\n",
    "print(vote_list)\n",
    "print(decision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
